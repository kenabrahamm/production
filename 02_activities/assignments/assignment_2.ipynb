{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assigment, we will work with the *Forest Fire* data set. Please download the data from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/162/forest+fires). Extract the data files into the subdirectory: `../data/fires/` (relative to `./05_src/`).\n",
    "\n",
    "## Objective\n",
    "\n",
    "+ The model objective is to predict the area affected by forest fires given the features set. \n",
    "+ The objective of this exercise is to assess your ability to construct and evaluate model pipelines.\n",
    "+ Please note: the instructions are not meant to be 100% prescriptive, but instead they are a set of minimum requirements. If you find predictive performance gains by applying additional steps, by all means show them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Description\n",
    "\n",
    "From the description file contained in the archive (`forestfires.names`), we obtain the following variable descriptions:\n",
    "\n",
    "1. X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "2. Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "3. month - month of the year: \"jan\" to \"dec\" \n",
    "4. day - day of the week: \"mon\" to \"sun\"\n",
    "5. FFMC - FFMC index from the FWI system: 18.7 to 96.20\n",
    "6. DMC - DMC index from the FWI system: 1.1 to 291.3 \n",
    "7. DC - DC index from the FWI system: 7.9 to 860.6 \n",
    "8. ISI - ISI index from the FWI system: 0.0 to 56.10\n",
    "9. temp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "10. RH - relative humidity in %: 15.0 to 100\n",
    "11. wind - wind speed in km/h: 0.40 to 9.40 \n",
    "12. rain - outside rain in mm/m2 : 0.0 to 6.4 \n",
    "13. area - the burned area of the forest (in ha): 0.00 to 1090.84 \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "### Specific Tasks\n",
    "\n",
    "+ Construct four model pipelines, out of combinations of the following components:\n",
    "\n",
    "    + Preprocessors:\n",
    "\n",
    "        - A simple processor that only scales numeric variables and recodes categorical variables.\n",
    "        - A transformation preprocessor that scales numeric variables and applies a non-linear transformation.\n",
    "    \n",
    "    + Regressor:\n",
    "\n",
    "        - A baseline regressor, which could be a [K-nearest neighbours model]() or a linear model like [Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) or [Ridge Regressors](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ridge_regression.html).\n",
    "        - An advanced regressor of your choice (e.g., Bagging, Boosting, SVR, etc.). TIP: select a tree-based method such that it does not take too long to run SHAP further below. \n",
    "\n",
    "+ Evaluate tune and evaluate each of the four model pipelines. \n",
    "\n",
    "    - Select a [performance metric](https://scikit-learn.org/stable/modules/linear_model.html) out of the following options: explained variance, max error, root mean squared error (RMSE), mean absolute error (MAE), r-squared.\n",
    "    - *TIPS*: \n",
    "    \n",
    "        * Out of the suggested metrics above, [some are correlation metrics, but this is a prediction problem](https://www.tmwr.org/performance#performance). Choose wisely (and don't choose the incorrect options.) \n",
    "\n",
    "+ Select the best-performing model and explain its predictions.\n",
    "\n",
    "    - Provide local explanations.\n",
    "    - Obtain global explanations and recommend a variable selection strategy.\n",
    "\n",
    "+ Export your model as a pickle file.\n",
    "\n",
    "\n",
    "You can work on the Jupyter notebook, as this experiment is fairly short (no need to use sacred). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Place the files in the ../../05_src/data/fires/ directory and load the appropriate file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/torch/utils/_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load the libraries as required.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, OneHotEncoder, PowerTransformer, QuantileTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import pickle\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random state for reproducibility\n",
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 517 entries, 0 to 516\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   coord_x  517 non-null    int64  \n",
      " 1   coord_y  517 non-null    int64  \n",
      " 2   month    517 non-null    object \n",
      " 3   day      517 non-null    object \n",
      " 4   ffmc     517 non-null    float64\n",
      " 5   dmc      517 non-null    float64\n",
      " 6   dc       517 non-null    float64\n",
      " 7   isi      517 non-null    float64\n",
      " 8   temp     517 non-null    float64\n",
      " 9   rh       517 non-null    int64  \n",
      " 10  wind     517 non-null    float64\n",
      " 11  rain     517 non-null    float64\n",
      " 12  area     517 non-null    float64\n",
      "dtypes: float64(8), int64(3), object(2)\n",
      "memory usage: 52.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "columns = [\n",
    "    'coord_x', 'coord_y', 'month', 'day', 'ffmc', 'dmc', 'dc', 'isi', 'temp', 'rh', 'wind', 'rain', 'area' \n",
    "]\n",
    "fires_dt = (pd.read_csv('../../05_src/data/fires/forestfires.csv', header = 0, names = columns))\n",
    "fires_dt.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get X and Y\n",
    "\n",
    "Create the features data frame and target data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = fires_dt.drop('area', axis=1)\n",
    "y = fires_dt['area']\n",
    "\n",
    "numerical_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (413, 12)\n",
      "Test set shape: (104, 12)\n",
      "Training target mean: 11.13\n",
      "Test target mean: 19.66\n",
      "\n",
      "Training target statistics:\n",
      "Zeros in training: 196 (47.5%)\n",
      "Non-zeros in training: 217 (52.5%)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Create two [Column Transformers](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html), called preproc1 and preproc2, with the following guidelines:\n",
    "\n",
    "- Numerical variables\n",
    "\n",
    "    * (Preproc 1 and 2) Scaling: use a scaling method of your choice (Standard, Robust, Min-Max). \n",
    "    * Preproc 2 only: \n",
    "        \n",
    "        + Choose a transformation for any of your input variables (or several of them). Evaluate if this transformation is convenient.\n",
    "        + The choice of scaler is up to you.\n",
    "\n",
    "- Categorical variables: \n",
    "    \n",
    "    * (Preproc 1 and 2) Apply [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) where appropriate.\n",
    "\n",
    "\n",
    "+ The only difference between preproc1 and preproc2 is the non-linear transformation of the numerical variables.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproc 1\n",
    "\n",
    "Create preproc1 below.\n",
    "\n",
    "+ Numeric: scaled variables, no other transforms.\n",
    "+ Categorical: one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preproc1 created successfully!\n",
      "Numerical features to be scaled: ['coord_x', 'coord_y', 'ffmc', 'dmc', 'dc', 'isi', 'temp', 'rh', 'wind', 'rain']\n",
      "Categorical features to be one-hot encoded: ['month', 'day']\n"
     ]
    }
   ],
   "source": [
    "# Preproc 1: Simple preprocessing with scaling and one-hot encoding\n",
    "preproc1 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', RobustScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "print(\"Preproc1 created successfully!\")\n",
    "print(\"Numerical features to be scaled:\", numerical_features)\n",
    "print(\"Categorical features to be one-hot encoded:\", categorical_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preproc 2\n",
    "\n",
    "Create preproc1 below.\n",
    "\n",
    "+ Numeric: scaled variables, non-linear transformation to one or more variables.\n",
    "+ Categorical: one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preproc2 created successfully!\n",
      "Numerical features will undergo power transformation + robust scaling\n",
      "Categorical features to be one-hot encoded: ['month', 'day']\n",
      "Power transformation (Yeo-Johnson) will make skewed variables more normal-like\n"
     ]
    }
   ],
   "source": [
    "# Preproc 2: Preprocessing with power transformation and scaling\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a pipeline that applies power transformation followed by robust scaling\n",
    "power_pipeline = make_pipeline(\n",
    "    PowerTransformer(method='yeo-johnson', standardize=False),\n",
    "    RobustScaler()\n",
    ")\n",
    "\n",
    "preproc2 = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', power_pipeline, numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'), categorical_features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "print(\"Preproc2 created successfully!\")\n",
    "print(\"Numerical features will undergo power transformation + robust scaling\")\n",
    "print(\"Categorical features to be one-hot encoded:\", categorical_features)\n",
    "print(\"Power transformation (Yeo-Johnson) will make skewed variables more normal-like\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pipeline\n",
    "\n",
    "\n",
    "Create a [model pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html): \n",
    "\n",
    "+ Add a step labelled `preprocessing` and assign the Column Transformer from the previous section.\n",
    "+ Add a step labelled `regressor` and assign a regression model to it. \n",
    "\n",
    "## Regressor\n",
    "\n",
    "+ Use a regression model to perform a prediction. \n",
    "\n",
    "    - Choose a baseline regressor, tune it (if necessary) using grid search, and evaluate it using cross-validation.\n",
    "    - Choose a more advance regressor, tune it (if necessary) using grid search, and evaluate it using cross-validation.\n",
    "    - Both model choices are up to you, feel free to experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline A created: preproc1 + Ridge Regression\n"
     ]
    }
   ],
   "source": [
    "# Pipeline A = preproc1 + baseline (Ridge Regression)\n",
    "pipeline_A = Pipeline([\n",
    "    ('preprocessing', preproc1),\n",
    "    ('regressor', Ridge(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "print(\"Pipeline A created: preproc1 + Ridge Regression\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline B created: preproc2 + Ridge Regression\n"
     ]
    }
   ],
   "source": [
    "# Pipeline B = preproc2 + baseline (Ridge Regression)\n",
    "pipeline_B = Pipeline([\n",
    "    ('preprocessing', preproc2),\n",
    "    ('regressor', Ridge(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "print(\"Pipeline B created: preproc2 + Ridge Regression\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline C created: preproc1 + Random Forest Regressor\n"
     ]
    }
   ],
   "source": [
    "# Pipeline C = preproc1 + advanced model (Random Forest)\n",
    "pipeline_C = Pipeline([\n",
    "    ('preprocessing', preproc1),\n",
    "    ('regressor', RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])\n",
    "\n",
    "print(\"Pipeline C created: preproc1 + Random Forest Regressor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline D created: preproc2 + Random Forest Regressor\n",
      "\n",
      "All 4 pipelines created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Pipeline D = preproc2 + advanced model (Random Forest)\n",
    "pipeline_D = Pipeline([\n",
    "    ('preprocessing', preproc2),\n",
    "    ('regressor', RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])\n",
    "\n",
    "print(\"Pipeline D created: preproc2 + Random Forest Regressor\")\n",
    "\n",
    "# Store all pipelines for easy access\n",
    "pipelines = {\n",
    "    'Pipeline_A': pipeline_A,\n",
    "    'Pipeline_B': pipeline_B, \n",
    "    'Pipeline_C': pipeline_C,\n",
    "    'Pipeline_D': pipeline_D\n",
    "}\n",
    "\n",
    "print(f\"\\nAll {len(pipelines)} pipelines created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune Hyperparams\n",
    "\n",
    "+ Perform GridSearch on each of the four pipelines. \n",
    "+ Tune at least one hyperparameter per pipeline.\n",
    "+ Experiment with at least four value combinations per pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Pipeline A (preproc1 + Ridge)\n",
    "param_grid_A = {'regressor__alpha': [0.1, 1.0, 10.0, 100.0]}\n",
    "\n",
    "grid_search_A = GridSearchCV(\n",
    "    pipeline_A, \n",
    "    param_grid_A, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Pipeline B (preproc2 + Ridge)\n",
    "param_grid_B = {'regressor__alpha': [0.1, 1.0, 10.0, 100.0]}\n",
    "\n",
    "grid_search_B = GridSearchCV(\n",
    "    pipeline_B, \n",
    "    param_grid_B, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Pipeline C (preproc1 + Random Forest)\n",
    "param_grid_C = { 'regressor__n_estimators': [50, 100, 200], 'regressor__max_depth': [5, 10, None]}\n",
    "\n",
    "grid_search_C = GridSearchCV(\n",
    "    pipeline_C, \n",
    "    param_grid_C, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Pipeline D (preproc2 + Random Forest)\n",
    "param_grid_D = {\n",
    "    'regressor__n_estimators': [50, 100, 200],\n",
    "    'regressor__max_depth': [5, 10, None]}\n",
    "\n",
    "grid_search_D = GridSearchCV(\n",
    "    pipeline_D, \n",
    "    param_grid_D, \n",
    "    cv=5, \n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1,\n",
    "    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Pipeline_A...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Pipeline_A - Best CV Score (neg_MSE): -2095.0220\n",
      "Pipeline_A - Best Parameters: {'regressor__alpha': 100.0}\n",
      "\n",
      "Fitting Pipeline_B...\n",
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "Pipeline_B - Best CV Score (neg_MSE): -2098.5730\n",
      "Pipeline_B - Best Parameters: {'regressor__alpha': 100.0}\n",
      "\n",
      "Fitting Pipeline_C...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline_C - Best CV Score (neg_MSE): -2709.9715\n",
      "Pipeline_C - Best Parameters: {'regressor__max_depth': None, 'regressor__n_estimators': 50}\n",
      "\n",
      "Fitting Pipeline_D...\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n",
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline_D - Best CV Score (neg_MSE): -2696.4242\n",
      "Pipeline_D - Best Parameters: {'regressor__max_depth': None, 'regressor__n_estimators': 50}\n",
      "\n",
      "Hyperparameter tuning completed for all pipelines!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/dsi_p/lib/python3.9/site-packages/sklearn/preprocessing/_encoders.py:202: UserWarning: Found unknown categories in columns [0] during transform. These unknown categories will be encoded as all zeros\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "grid_searches = {\n",
    "    'Pipeline_A': grid_search_A,\n",
    "    'Pipeline_B': grid_search_B,\n",
    "    'Pipeline_C': grid_search_C,\n",
    "    'Pipeline_D': grid_search_D\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, grid_search in grid_searches.items():\n",
    "    print(f\"Fitting {name}...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    results[name] = {\n",
    "        'best_score': grid_search.best_score_,\n",
    "        'best_params': grid_search.best_params_,\n",
    "        'best_estimator': grid_search.best_estimator_\n",
    "    }\n",
    "    print(f\"{name} - Best CV Score (neg_MSE): {grid_search.best_score_:.4f}\")\n",
    "    print(f\"{name} - Best Parameters: {grid_search.best_params_}\\n\")\n",
    "\n",
    "print(\"Hyperparameter tuning completed for all pipelines!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline_A Test Results:\n",
      "  RMSE: 108.4377\n",
      "  MAE: 24.1443\n",
      "  R2 Score: 0.0025\n",
      "  Best Parameters: {'regressor__alpha': 100.0}\n",
      "\n",
      "Pipeline_B Test Results:\n",
      "  RMSE: 108.3756\n",
      "  MAE: 24.0966\n",
      "  R2 Score: 0.0036\n",
      "  Best Parameters: {'regressor__alpha': 100.0}\n",
      "\n",
      "Pipeline_C Test Results:\n",
      "  RMSE: 109.4042\n",
      "  MAE: 26.5349\n",
      "  R2 Score: -0.0154\n",
      "  Best Parameters: {'regressor__max_depth': None, 'regressor__n_estimators': 50}\n",
      "\n",
      "Pipeline_D Test Results:\n",
      "  RMSE: 110.4351\n",
      "  MAE: 26.6906\n",
      "  R2 Score: -0.0346\n",
      "  Best Parameters: {'regressor__max_depth': None, 'regressor__n_estimators': 50}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_results = {}\n",
    "for name, result in results.items():\n",
    "    best_model = result['best_estimator']\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    test_results[name] = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'CV_Score': result['best_score'],\n",
    "        'best_params': result['best_params']\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Test Results:\")\n",
    "    print(f\"  RMSE: {rmse:.4f}\")\n",
    "    print(f\"  MAE: {mae:.4f}\")\n",
    "    print(f\"  R2 Score: {r2:.4f}\")\n",
    "    print(f\"  Best Parameters: {result['best_params']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "+ Which model has the best performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export\n",
    "\n",
    "+ Save the best performing model to a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the best performing model (Pipeline_A) to a pickle file\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../../05_src/models', exist_ok=True)\n",
    "\n",
    "# Get the best model (Pipeline_A)\n",
    "best_pipeline_name = 'Pipeline_A'  # Based on our analysis above\n",
    "best_model = results[best_pipeline_name]['best_estimator']\n",
    "\n",
    "# Save the best model\n",
    "model_filename = '../../05_src/models/best_forest_fire_model.pkl'\n",
    "with open(model_filename, 'wb') as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "print(f\"✅ Best model ({best_pipeline_name}) saved to: {model_filename}\")\n",
    "\n",
    "# Also save model metadata for future reference\n",
    "model_info = {\n",
    "    'pipeline_name': best_pipeline_name,\n",
    "    'model': best_model,\n",
    "    'test_metrics': test_results[best_pipeline_name],\n",
    "    'cv_score': results[best_pipeline_name]['best_score'],\n",
    "    'best_params': results[best_pipeline_name]['best_params'],\n",
    "    'feature_names': list(X.columns),\n",
    "    'numerical_features': numerical_features,\n",
    "    'categorical_features': categorical_features,\n",
    "    'preprocessing': 'RobustScaler + OneHotEncoder',\n",
    "    'algorithm': 'Ridge Regression'\n",
    "}\n",
    "\n",
    "metadata_filename = '../../05_src/models/model_metadata.pkl'\n",
    "with open(metadata_filename, 'wb') as f:\n",
    "    pickle.dump(model_info, f)\n",
    "\n",
    "print(f\"✅ Model metadata saved to: {metadata_filename}\")\n",
    "print(f\"\\nModel Performance Summary:\")\n",
    "print(f\"  - Algorithm: Ridge Regression\")\n",
    "print(f\"  - Preprocessing: Simple scaling + one-hot encoding\")\n",
    "print(f\"  - Best Parameters: {results[best_pipeline_name]['best_params']}\")\n",
    "print(f\"  - Test RMSE: {test_results[best_pipeline_name]['RMSE']:.4f}\")\n",
    "print(f\"  - Test R²: {test_results[best_pipeline_name]['R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Explanations for Pipeline_A (Best Model)\n",
    "\n",
    "# Get the best model for SHAP analysis\n",
    "best_model = results['Pipeline_A']['best_estimator']\n",
    "\n",
    "# Transform the data using the preprocessing pipeline\n",
    "X_train_transformed = best_model.named_steps['preprocessing'].transform(X_train)\n",
    "X_test_transformed = best_model.named_steps['preprocessing'].transform(X_test)\n",
    "\n",
    "# Get feature names after preprocessing\n",
    "preprocessor = best_model.named_steps['preprocessing']\n",
    "feature_names = []\n",
    "\n",
    "# Get numerical feature names (same as original)\n",
    "feature_names.extend(numerical_features)\n",
    "\n",
    "# Get categorical feature names after one-hot encoding\n",
    "cat_transformer = preprocessor.named_transformers_['cat']\n",
    "cat_feature_names = cat_transformer.get_feature_names_out(categorical_features)\n",
    "feature_names.extend(cat_feature_names)\n",
    "\n",
    "print(f\"Total features after preprocessing: {len(feature_names)}\")\n",
    "print(f\"Feature names: {feature_names}\")\n",
    "\n",
    "# Create SHAP explainer for the Ridge regression model\n",
    "explainer = shap.Explainer(best_model.named_steps['regressor'], X_train_transformed, feature_names=feature_names)\n",
    "\n",
    "# Calculate SHAP values for test set (using a sample to speed up computation)\n",
    "sample_size = min(50, len(X_test_transformed))  # Use up to 50 test samples\n",
    "X_test_sample = X_test_transformed[:sample_size]\n",
    "shap_values = explainer(X_test_sample)\n",
    "\n",
    "print(f\"\\n📊 SHAP Analysis completed for {sample_size} test samples\")\n",
    "\n",
    "# 1. Local explanation for a specific observation\n",
    "sample_idx = 0  # First test observation\n",
    "sample_prediction = best_model.predict(X_test.iloc[[sample_idx]])[0]\n",
    "actual_value = y_test.iloc[sample_idx]\n",
    "\n",
    "print(f\"\\n🔍 LOCAL EXPLANATION (Test Sample {sample_idx}):\")\n",
    "print(f\"  - Actual fire area: {actual_value:.2f} ha\")\n",
    "print(f\"  - Predicted fire area: {sample_prediction:.2f} ha\")\n",
    "print(f\"  - Prediction error: {abs(actual_value - sample_prediction):.2f} ha\")\n",
    "\n",
    "# Show top contributing features for this sample\n",
    "sample_shap = shap_values[sample_idx]\n",
    "feature_contributions = [(feature_names[i], sample_shap.values[i]) for i in range(len(feature_names))]\n",
    "feature_contributions.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "print(f\"\\n  Top 5 most important features for this prediction:\")\n",
    "for i, (feature, contribution) in enumerate(feature_contributions[:5]):\n",
    "    direction = \"increases\" if contribution > 0 else \"decreases\"\n",
    "    print(f\"  {i+1}. {feature}: {contribution:.3f} ({direction} prediction)\")\n",
    "\n",
    "# 2. Global feature importance\n",
    "print(f\"\\n🌐 GLOBAL FEATURE IMPORTANCE:\")\n",
    "print(\"  Features ranked by average absolute SHAP value across all test samples:\")\n",
    "\n",
    "# Calculate mean absolute SHAP values\n",
    "mean_shap_values = np.abs(shap_values.values).mean(axis=0)\n",
    "global_importance = [(feature_names[i], mean_shap_values[i]) for i in range(len(feature_names))]\n",
    "global_importance.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n  Most important features (top 8):\")\n",
    "for i, (feature, importance) in enumerate(global_importance[:8]):\n",
    "    print(f\"  {i+1}. {feature}: {importance:.4f}\")\n",
    "\n",
    "print(f\"\\n  Least important features (bottom 5):\")\n",
    "for i, (feature, importance) in enumerate(global_importance[-5:], len(global_importance)-4):\n",
    "    print(f\"  {i}. {feature}: {importance:.4f}\")\n",
    "\n",
    "# 3. Feature selection recommendations\n",
    "print(f\"\\n💡 FEATURE SELECTION RECOMMENDATIONS:\")\n",
    "\n",
    "# Identify very low importance features\n",
    "low_importance_threshold = 0.01\n",
    "low_importance_features = [feat for feat, imp in global_importance if imp < low_importance_threshold]\n",
    "\n",
    "if low_importance_features:\n",
    "    print(f\"\\n  Features that could be considered for removal (importance < {low_importance_threshold}):\")\n",
    "    for feat in low_importance_features:\n",
    "        print(f\"  - {feat}\")\n",
    "    \n",
    "    print(f\"\\n  🧪 How to test feature removal:\")\n",
    "    print(f\"  1. Create a new pipeline excluding these {len(low_importance_features)} features\")\n",
    "    print(f\"  2. Use cross-validation to compare RMSE with/without these features\")\n",
    "    print(f\"  3. If RMSE doesn't significantly increase, remove them for model simplicity\")\n",
    "    print(f\"  4. Consider statistical tests (e.g., permutation importance) for validation\")\n",
    "else:\n",
    "    print(f\"  All features show importance ≥ {low_importance_threshold}\")\n",
    "    print(f\"  Consider removing features with importance < {global_importance[-1][1]:.4f} (lowest)\")\n",
    "\n",
    "print(f\"\\n  📈 Feature enhancement testing strategy:\")\n",
    "print(f\"  1. Use recursive feature elimination (RFE) with the Ridge model\")\n",
    "print(f\"  2. Compare nested cross-validation scores with different feature subsets\")\n",
    "print(f\"  3. Apply statistical feature selection (f_regression, mutual_info_regression)\")\n",
    "print(f\"  4. Test feature interactions (polynomial features) for top predictors\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\n📋 SHAP ANALYSIS SUMMARY:\")\n",
    "print(f\"  - Model: Ridge Regression (alpha={results['Pipeline_A']['best_params']['regressor__alpha']})\")\n",
    "print(f\"  - Total features: {len(feature_names)}\")\n",
    "print(f\"  - Most important: {global_importance[0][0]} (SHAP value: {global_importance[0][1]:.4f})\")\n",
    "print(f\"  - Least important: {global_importance[-1][0]} (SHAP value: {global_importance[-1][1]:.4f})\")\n",
    "print(f\"  - Features with importance < 0.01: {len(low_importance_features)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explain\n",
    "\n",
    "+ Use SHAP values to explain the following only for the best-performing model:\n",
    "\n",
    "    - Select an observation in your test set and explain which are the most important features that explain that observation's specific prediction.\n",
    "\n",
    "    - In general, across the complete training set, which features are the most and least important.\n",
    "\n",
    "+ If you were to remove features from the model, which ones would you remove? Why? How would you test that these features are actually enhancing model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criteria\n",
    "\n",
    "The [rubric](./assignment_2_rubric_clean.xlsx) contains the criteria for assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Information\n",
    "\n",
    "🚨 **Please review our [Assignment Submission Guide](https://github.com/UofT-DSI/onboarding/blob/main/onboarding_documents/submissions.md)** 🚨 for detailed instructions on how to format, branch, and submit your work. Following these guidelines is crucial for your submissions to be evaluated correctly.\n",
    "\n",
    "### Submission Parameters:\n",
    "* Submission Due Date: `HH:MM AM/PM - DD/MM/YYYY`\n",
    "* The branch name for your repo should be: `assignment-2`\n",
    "* What to submit for this assignment:\n",
    "    * This Jupyter Notebook (assignment_2.ipynb) should be populated and should be the only change in your pull request.\n",
    "* What the pull request link should look like for this assignment: `https://github.com/<your_github_username>/production/pull/<pr_id>`\n",
    "    * Open a private window in your browser. Copy and paste the link to your pull request into the address bar. Make sure you can see your pull request properly. This helps the technical facilitator and learning support staff review your submission easily.\n",
    "\n",
    "Checklist:\n",
    "- [ ] Created a branch with the correct naming convention.\n",
    "- [ ] Ensured that the repository is public.\n",
    "- [ ] Reviewed the PR description guidelines and adhered to them.\n",
    "- [ ] Verify that the link is accessible in a private browser window.\n",
    "\n",
    "If you encounter any difficulties or have questions, please don't hesitate to reach out to our team via our Slack at the `help` channel. Our Technical Facilitators and Learning Support staff are here to help you navigate any challenges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "Cortez,Paulo and Morais,Anbal. (2008). Forest Fires. UCI Machine Learning Repository. https://doi.org/10.24432/C5D88D."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_p",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
